{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "touched-logic",
   "metadata": {},
   "source": [
    "# Theory/Computation Problems\n",
    "\n",
    "### Problem 1 (20 points) \n",
    "Show that the stationary point (zero gradient) of the function\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    f=2x_{1}^{2} - 4x_1 x_2+ 1.5x^{2}_{2}+ x_2\n",
    "\\end{aligned}\n",
    "$$\n",
    "is a saddle (with indefinite Hessian). Find the directions of downslopes away from the saddle. Hint: Use Taylor's expansion at the saddle point. Find directions that reduce $f$.\n",
    "\n",
    "### Problem 2 (50 points) \n",
    "\n",
    "* (10 points) Find the point in the plane $x_1+2x_2+3x_3=1$ in $\\mathbb{R}^3$ that is nearest to the point $(-1,0,1)^T$. Is this a convex problem? Hint: Convert the problem into an unconstrained problem using $x_1+2x_2+3x_3=1$.\n",
    "\n",
    "* (40 points) Implement the gradient descent and Newton's algorithm for solving the problem. Attach your codes along with a short summary including (1) the initial points tested, (2) corresponding solutions, (3) a log-linear convergence plot.\n",
    "\n",
    "### Problem 3 (10 points) \n",
    "Let $f(x)$ and $g(x)$ be two convex functions defined on the convex set $\\mathcal{X}$. \n",
    "* (5 points) Prove that $af(x)+bg(x)$ is convex for $a>0$ and $b>0$. \n",
    "* (5 points) In what conditions will $f(g(x))$ be convex?\n",
    "\n",
    "### Problem 4 (bonus 10 points)\n",
    "Show that $f({\\bf x}_1) \\geq f(\\textbf{x}_0) + \n",
    "    \\textbf{g}_{\\textbf{x}_0}^T(\\textbf{x}_1-\\textbf{x}_0)$ for a convex function $f(\\textbf{x}): \\mathcal{X} \\rightarrow \\mathbb{R}$ and for $\\textbf{x}_0$, $\\textbf{x}_1 \\in \\mathcal{X}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-carbon",
   "metadata": {},
   "source": [
    "# Design Problems\n",
    "\n",
    "### Problem 5 (20 points) \n",
    "Consider an illumination problem: There are $n$ lamps and $m$ mirrors fixed to the ground. The target reflection intensity level is $I_t$. The actual reflection intensity level on the $k$th mirror can be computed as $\\textbf{a}_k^T \\textbf{p}$, where $\\textbf{a}_k$ is given by the distances between all lamps to the mirror, and $\\textbf{p}:=[p_1,...,p_n]^T$ are the power output of the lamps. The objective is to keep the actual intensity levels as close to the target as possible by tuning the power output $\\textbf{p}$.\n",
    "\n",
    "* (5 points) Formulate this problem as an optimization problem. \n",
    "* (5 points) Is your problem convex?\n",
    "* (5 points) If we require the overall power output of any of the $n$ lamps to be less than $p^*$, will the problem have a unique solution?\n",
    "* (5 points) If we require no more than half of the lamps to be switched on, will the problem have a unique solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-twins",
   "metadata": {},
   "source": [
    "# Note\n",
    "\n",
    "For this homework, you may want to attach sketches as means to explain your ideas. Here is how you can attach images.\n",
    "\n",
    "![everly1](img/everly7.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "moving-prescription",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.25,\n",
       " 0.4375,\n",
       " 0.578125,\n",
       " 0.68359375,\n",
       " 0.7626953125,\n",
       " 0.822021484375,\n",
       " 0.86651611328125,\n",
       " 0.8998870849609375,\n",
       " 0.9249153137207031,\n",
       " 0.9436864852905273,\n",
       " 0.9577648639678955,\n",
       " 0.9683236479759216,\n",
       " 0.9762427359819412,\n",
       " 0.9821820519864559,\n",
       " 0.9866365389898419,\n",
       " 0.9899774042423815,\n",
       " 0.9924830531817861,\n",
       " 0.9943622898863396,\n",
       " 0.9957717174147547,\n",
       " 0.996828788061066,\n",
       " 0.9976215910457995,\n",
       " 0.9982161932843496,\n",
       " 0.9986621449632622,\n",
       " 0.9989966087224467,\n",
       " 0.999247456541835,\n",
       " 0.9994355924063762,\n",
       " 0.9995766943047821]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample code for Problem 2\n",
    "\n",
    "obj = lambda x: (x - 1)**2  # note that this is 1D. In Prob 2 it should be 2D.\n",
    "grad = lambda x, y: 2*(x - 1)  # this is not the correct gradient!\n",
    "eps = 1e-3  # termination criterion\n",
    "x0 = 0.  # initial guess\n",
    "k = 0  # counter\n",
    "soln = [x0]  # use an array to store the search steps\n",
    "x = soln[k]  # start with the initial guess\n",
    "error = abs(grad(x, x2))  # compute the error. Note you will need to compute the norm for 2D grads, rather than the absolute value\n",
    "# a = 0.01  # set a fixed step size to start with\n",
    "\n",
    "# Armijo line search\n",
    "def line_search(x):\n",
    "    a = 1.  # initialize step size\n",
    "    phi = lambda a, x: obj(x) - a*0.8*grad(x)**2  # define phi as a search criterion\n",
    "    while phi(a,x)<obj(x-a*grad(x)):  # if f(x+a*d)>phi(a) then backtrack. d is the search direction\n",
    "        a = 0.5*a\n",
    "    return a\n",
    "\n",
    "while error >= eps:  # keep searching while gradient norm is larger than eps\n",
    "    a = line_search(x)\n",
    "    x = x - a*grad(x)\n",
    "    soln.append(x)\n",
    "    error = abs(grad(x))\n",
    "    \n",
    "soln  # print the search trajectory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02c3381b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10, 10],\n",
       " [-0.14269210976135316, 0.7856215185424826],\n",
       " [-0.14269210976135316, 0.7856215185424826],\n",
       " [-0.14269210976135316, 0.7856215185424826],\n",
       " [-0.14269210976135316, 0.7856215185424826],\n",
       " [-0.14269210976135316, 0.7856215185424826],\n",
       " [-0.14269210976135316, 0.7856215185424826],\n",
       " [-0.14269210976135316, 0.7856215185424826],\n",
       " [-0.14269210976135316, 0.7856215185424826],\n",
       " [-0.14269210976135316, 0.7856215185424826],\n",
       " [-0.14269210976135316, 0.7856215185424826],\n",
       " [-0.14269210976135316, 0.7856215185424826],\n",
       " [-0.14269210976135316, 0.7856215185424826],\n",
       " [-0.14269210976135316, 0.7856215185424826],\n",
       " [-0.14269210976135316, 0.7856215185424826],\n",
       " [-0.14269210976135316, 0.7856215185424826],\n",
       " [-0.14269210976135316, 0.7856215185424826],\n",
       " [-0.14269210976135316, 0.7856215185424826],\n",
       " [-0.14269210976135316, 0.7856215185424826],\n",
       " [-0.14269210976135316, 0.7856215185424826],\n",
       " [-0.14269210976135316, 0.7856215185424826],\n",
       " [-0.14269210976135316, 0.7856215185424826],\n",
       " [-0.14269210976135316, 0.7856215185424826],\n",
       " [-0.14269210976135316, 0.7856215185424826],\n",
       " [-0.14269210976135316, 0.7856215185424826],\n",
       " [-0.14269210976135316, 0.7856215185424826],\n",
       " [-0.14269210976135316, 0.7856215185424826],\n",
       " [-0.14269210976135316, 0.7856215185424826],\n",
       " [-0.14269210976135316, 0.7856215185424826],\n",
       " [-0.14269210976135316, 0.7856215185424826],\n",
       " [-0.14269210976135316, 0.7856215185424826],\n",
       " [-0.14269210976135316, 0.7856215185424826],\n",
       " [-0.14269210976135316, 0.7856215185424826]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "obj = lambda x: 5*x[0]**2 + 12*x[0]*x[1] - 8*x[0] + 10*x[1]**2 - 14*x[1] + 6\n",
    "\n",
    "def grad(x) :\n",
    "    return [10*x[0] + 12*x[1] - 8, 12*x[0] + 20*x[1] - 14]\n",
    "\n",
    "eps = 1e-3\n",
    "\n",
    "xin = 10\n",
    "\n",
    "\n",
    "n = 0\n",
    "\n",
    "soln = [[xin, xin]]\n",
    "\n",
    "x = [xin, xin]\n",
    "\n",
    "error = numpy.linalg.norm(grad(x))\n",
    "\n",
    "def line_search(x) :\n",
    "    a = 1.\n",
    "    phi = lambda a, x: [obj(x) - a*.8*grad(x)[0]**2, obj(x) - a*.8*grad(x)[1]**2]\n",
    "    #print(([x[0]-a*grad(x)[0],x[1]-a*grad(x)[1]]))\n",
    "    while phi(a,x)[0]<obj([x[0]-a*grad(x)[0],x[1]-a*grad(x)[1]]) and phi(a,x)[1]<obj([x[0]-a*grad(x)[0],x[1]-a*grad(x)[1]]):\n",
    "        \n",
    "        a = .5*a\n",
    "        \n",
    "    return a\n",
    "    \n",
    "while error >= eps :\n",
    "    \n",
    "    a = line_search(x)\n",
    "    x[0] = x[0] - a*grad(x)[0]\n",
    "    x[1] = x[1] - a*grad(x)[1]\n",
    "    soln.append(x)\n",
    "    error = numpy.linalg.norm(grad(x))\n",
    " \n",
    "\n",
    "    \n",
    "soln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a28e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8f8902",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
